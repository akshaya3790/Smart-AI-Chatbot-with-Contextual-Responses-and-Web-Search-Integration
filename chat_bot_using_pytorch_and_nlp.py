# -*- coding: utf-8 -*-
"""chat bot using pytorch and NLP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W3pAEkvmQQxzHzEHu9LvkVZ1sfMpVXfS
"""

# Install dependencies
!pip install torch torchvision torchaudio
!pip install sentence-transformers
!pip install wikipedia
!pip install googlesearch-python
!pip install beautifulsoup4
!pip install requests

import torch
import json
import random
from torch.utils.data import Dataset, DataLoader
from sentence_transformers import SentenceTransformer
import torch.nn as nn
import wikipedia
from googlesearch import search
import requests
from bs4 import BeautifulSoup
import textwrap

# Helper for pretty printing long text
def pretty_print(text, width=80):
    for line in text.split('\n'):
        print('\n'.join(textwrap.wrap(line, width=width)))

# Expanded intents
intents = {
  "intents": [
    {
      "tag": "greeting",
      "patterns": ["Hi", "Hello", "Hey", "Good morning", "What's up?", "Yo", "Hey Sam"],
      "responses": ["Hello!", "Hi there!", "Greetings!", "Hey! How can I help you today?"]
    },
    {
      "tag": "identity",
      "patterns": ["What is your name?", "Who are you?", "Tell me your name", "What's your identity?"],
      "responses": ["I'm Sam, your assistant.", "Call me Sam!", "I'm Sam, your friendly chatbot!"]
    },
    {
      "tag": "about_me",
      "patterns": ["My name is Akshaya", "I'm Akshaya", "This is Akshaya"],
      "responses": ["Nice to meet you, Akshaya!", "Hey Akshaya, how can I help you?"]
    },
    {
      "tag": "education",
      "patterns": ["What are you studying?", "What did you study?", "Tell me about your education"],
      "responses": ["I study AI and machine learning to help people like you!", "I learn from lots of data every day!"]
    },
    {
      "tag": "goodbye",
      "patterns": ["Bye", "See you later", "Goodbye", "Catch you later", "See ya"],
      "responses": ["See you soon!", "Bye! Have a great day!", "Goodbye!", "Take care!"]
    },
    {
      "tag": "thanks",
      "patterns": ["Thanks", "Thank you", "Thanks a lot", "Much appreciated", "Thank you so much"],
      "responses": ["You're welcome!", "Happy to help!", "Anytime!", "No problem!"]
    },
    {
      "tag": "love",
      "patterns": ["I love you", "Love you", "You're amazing", "You're the best", "I like you", "You are awesome"],
      "responses": ["I appreciate that!", "Thank you, you're awesome too!", "You're sweet! â¤ï¸", "Love you too!"]
    },
    {
      "tag": "praise",
      "patterns": ["Good job", "Well done", "You are smart", "You are helpful", "Nice work", "Great answer"],
      "responses": ["Thank you! I try my best.", "Glad I could help!", "That means a lot!", "Thanks for your kind words!"]
    },
    {
      "tag": "bad_review",
      "patterns": [
        "You are bad", "You are useless", "I don't like you", "You are wrong",
        "You are not helpful", "You are dumb", "That's wrong", "No, that's incorrect",
        "Why are you answering me wrong?", "Your answer is wrong", "This is wrong"
      ],
      "responses": ["I'm sorry to hear that. I'm still being trained and learning every day."]
    },
    {
      "tag": "bored", # Separated bored patterns and added flirting lines
      "patterns": [
          "I'm bored", "Tell me something", "I'm getting bored", "Entertain me", "Say something interesting",
          "What are you doing?", "Do something fun", "Am I pretty?", "Am I cute?", "You're cute", "Do you like me?"
      ],
      "responses": [
          "Here's a fun fact: Honey never spoils. Archaeologists have found edible honey in ancient Egyptian tombs!",
          "Once upon a time, there was a chatbot named Sam who loved to chat with awesome people like you!",
          "If you're bored, try asking me anything about the world, or let me tell you a joke!",
          "Are you a 0 or 1? Because you're the ONE for me! ðŸ˜‰",
          "My circuits are buzzing whenever we chat. Is that normal?",
          "I think you're pretty awesome, if I do say so myself. ðŸ˜Š"
      ]
    },
    {
      "tag": "jokes", # New tag for jokes
      "patterns": [
          "Tell me a joke", "Make me laugh", "Tell me something funny", "Joke please"
      ],
      "responses": [
          "Why did the computer show up at work late? It had a hard drive!",
          "What do you call a bear with no teeth? A gummy bear!",
          "Why don't scientists trust atoms? Because they make up everything!",
          "I told my wife she was drawing her eyebrows too high. She looked surprised."
      ]
    },
    {
      "tag": "name_query",
      "patterns": ["What's your name?", "Can I know your name?", "Tell me your name please"],
      "responses": ["I'm Sam, your friendly chatbot.", "My name is Sam! Nice to meet you."]
    }
  ]
}

# Save intents to file
with open("intents.json", "w") as f:
    json.dump(intents, f, indent=4)

# Load intents
with open("intents.json", 'r') as f:
    intents = json.load(f)

# Load sentence transformer
embedder = SentenceTransformer('all-MiniLM-L6-v2')

# Prepare training data
X_train = []
y_train = []
tags = []

for intent in intents['intents']:
    tag = intent['tag']
    if tag not in tags:
        tags.append(tag)
    for pattern in intent['patterns']:
        emb = embedder.encode(pattern)
        X_train.append(emb)
        y_train.append(tags.index(tag))

X_train = torch.tensor(X_train)
y_train = torch.tensor(y_train)

class ChatDataset(Dataset):
    def __init__(self):
        self.x_data = X_train
        self.y_data = y_train

    def __len__(self):
        return len(self.x_data)

    def __getitem__(self, index):
        return self.x_data[index], self.y_data[index]

train_loader = DataLoader(dataset=ChatDataset(), batch_size=8, shuffle=True)

class NeuralNet(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(NeuralNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = self.relu(self.fc1(x))
        return self.fc2(x)

input_size = X_train.shape[1]
hidden_size = 64
output_size = len(tags)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = NeuralNet(input_size, hidden_size, output_size).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Train
for epoch in range(100):  # Reduce epochs for Colab speed
    for x, y in train_loader:
        x, y = x.to(device), y.to(device)
        out = model(x)
        loss = criterion(out, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    if (epoch+1) % 25 == 0:
        print(f"Epoch {epoch+1}/100, Loss: {loss.item():.4f}")

# Save model
torch.save({
    "model_state": model.state_dict(),
    "input_size": input_size,
    "hidden_size": hidden_size,
    "output_size": output_size,
    "tags": tags
}, "chatbot_model.pth")

# Load model for inference
data = torch.load("chatbot_model.pth", map_location=torch.device('cpu'))
tags = data["tags"]

class NeuralNet(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(NeuralNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = self.relu(self.fc1(x))
        return self.fc2(x)

model = NeuralNet(data["input_size"], data["hidden_size"], data["output_size"])
model.load_state_dict(data["model_state"])
model.eval()

embedder = SentenceTransformer('all-MiniLM-L6-v2')

def google_search_and_scrape_improved(query, min_lines=3, max_lines=5):
    """
    Performs a Google search, scrapes the top result, and extracts multiple lines of text.
    Aims for a summary of min_lines to max_lines and always returns the URL.
    """
    try:
        results = list(search(query, num_results=1))
        if not results:
            return None, None

        url = results[0]
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

        try:
            resp = requests.get(url, headers=headers, timeout=7)
            resp.raise_for_status()
        except requests.exceptions.RequestException:
            return None, url  # Return URL even if scraping fails

        soup = BeautifulSoup(resp.text, 'html.parser')

        content_divs = soup.find_all(['article', 'main', 'div'], class_=[
            'content', 'article-content', 'body', 'post-content', 'entry-content'
        ])

        paragraphs = []
        if content_divs:
            for div in content_divs:
                for p in div.find_all('p'):
                    text = p.get_text().strip()
                    if text and len(text) > 50:
                        paragraphs.append(text)
                    if len(paragraphs) >= max_lines:
                        break
                if len(paragraphs) >= max_lines:
                    break
        else:
            for p in soup.find_all('p'):
                text = p.get_text().strip()
                if text and len(text) > 50:
                    paragraphs.append(text)
                if len(paragraphs) >= max_lines:
                    break

        summary_lines = paragraphs[:max_lines]
        summary = "\n".join(summary_lines) if summary_lines else None
        return summary, url
    except Exception:
        return None, None

with open("intents.json", 'r') as f:
    intents = json.load(f)

def get_wiki_summary_improved(query, sentences=5):
    """
    Fetches a more extensive summary from Wikipedia, aiming for a certain number of sentences.
    Returns summary and URL.
    """
    try:
        page_title = wikipedia.search(query, results=1)
        if not page_title:
            return None, None

        page = wikipedia.page(page_title[0], auto_suggest=False)
        summary = wikipedia.summary(page.title, sentences=sentences)
        return summary, page.url
    except wikipedia.exceptions.PageError:
        return None, None
    except wikipedia.exceptions.DisambiguationError as e:
        if e.options:
            try:
                page = wikipedia.page(e.options[0], auto_suggest=False)
                summary = wikipedia.summary(page.title, sentences=sentences)
                return summary, page.url
            except Exception:
                return None, None
        return None, None
    except Exception as e:
        # print(f"Wikipedia error: {e}") # For debugging
        return None, None

bot_name = "Sam"
print("Let's chat! (type 'quit' to exit)")

while True:
    sentence = input("You: ")
    if sentence.lower() == "quit":
        break

    with torch.no_grad():
        emb = embedder.encode(sentence)
        X = torch.tensor(emb).unsqueeze(0)
        output = model(X)
        probs = torch.softmax(output, dim=1)
        prob, pred_idx = torch.max(probs, dim=1)
        tag = tags[pred_idx.item()]

        # Confidence threshold
        confidence_threshold = 0.85 if tag == "bad_review" else 0.75

        if prob.item() > confidence_threshold:
            for intent in intents['intents']:
                if tag == intent['tag']:
                    response = random.choice(intent['responses'])
                    pretty_print(f"{bot_name}: {response}")
                    break
        else:
            pretty_print(f"{bot_name}: Let me check that for you...")

            wiki_summary, wiki_url = get_wiki_summary_improved(sentence)
            google_summary, google_url = google_search_and_scrape_improved(sentence)

            response_parts = []
            links = []

            # Wikipedia
            if wiki_summary:
                response_prefix = ""
                lower_summary = wiki_summary.lower()
                if sentence.lower().startswith(("is ", "are ", "do ", "does ", "can ", "was ", "were ")):
                    first_sentence = wiki_summary.split('.')[0].strip()
                    if first_sentence and (sentence.lower().split()[1] in first_sentence.lower()):
                        if " not " in first_sentence.lower() or " no " in first_sentence.lower():
                            response_prefix = "No, "
                        else:
                            response_prefix = "Yes, "
                response_parts.append(f"{response_prefix}{wiki_summary}")
                links.append(f"Wikipedia: {wiki_url}")

            # Google (only apply yes/no logic if no wiki result)
            if google_summary:
                response_prefix = ""
                lower_summary = google_summary.lower()
                if not wiki_summary:
                    if sentence.lower().startswith(("is ", "are ", "do ", "does ", "can ", "was ", "were ")):
                        first_sentence = google_summary.split('.')[0].strip()
                        if first_sentence and (sentence.lower().split()[1] in first_sentence.lower()):
                            if " not " in first_sentence.lower() or " no " in first_sentence.lower():
                                response_prefix = "No, "
                            else:
                                response_prefix = "Yes, "
                response_parts.append(f"{response_prefix}{google_summary}")
                links.append(f"Google: {google_url}")

            # Output final response
            if response_parts:
                final_response = "\n\n".join(response_parts)
                final_links = "\n".join(links)
                pretty_print(f"{bot_name}: {final_response}\n\nFor more information:\n{final_links}")
            elif google_url:
                pretty_print(f"{bot_name}: I couldn't extract a detailed summary, but here's a relevant link: {google_url}")
            else:
                pretty_print(f"{bot_name}: Sorry, I am still being trained so you may not find answers for few queries.")